{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262cc58-6c95-4fad-8da8-601902413c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee0c04-18be-480a-be75-d6bc2078e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==5.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36067944-770a-42c5-8965-d514679a3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df7405b-7af1-4074-ae6d-434cfd27b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import TokenizersBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3c131f-a7f4-4400-93af-53f247ba9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu128\n",
      "PyTorch CUDA Version: 12.8\n",
      "CuDNN Version:        91002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"CuDNN Version:        {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d50a77-a1c7-4191-8ec7-71dacf24d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import FineGrainedFP8Config\n",
    "from transformers import Mistral3ForConditionalGeneration, Mistral3Config, PixtralVisionConfig, MistralConfig\n",
    "\n",
    "#  4-Bit Quantization Config \n",
    "bnb4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "bnb8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# use for 8B model\n",
    "q8_config = FineGrainedFP8Config()\n",
    "\n",
    "\n",
    "def print_model_specs(model):\n",
    "    # 1. Get the number of parameters\n",
    "    # set only_trainable=False to include frozen weights (e.g. if using LoRA)\n",
    "    total_params = model.num_parameters(only_trainable=False)\n",
    "    \n",
    "    # 2. Get the memory footprint (size of weights in VRAM/RAM)\n",
    "    # This method is specific to Hugging Face models\n",
    "    memory_bytes = model.get_memory_footprint()\n",
    "    \n",
    "    # Format for readability\n",
    "    print(f\"Model Parameters: {total_params / 1_000_000_000:.2f} Billion\")\n",
    "    print(f\"Memory Footprint: {memory_bytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9954edad-d5e7-4fd3-a6ca-60dc8d2addec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Detected: NVIDIA GeForce RTX 4070 Ti\n",
      "\n",
      "Loading mistralai/Ministral-3-14B-Instruct-2512-BF16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278a842675d04ca3abf203dbc89654af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd86643401149618c05f16815a620fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d73a0be6b2c4954b8e2b21b26e886ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 136.81 MiB is free. Including non-PyTorch memory, this process has 11.43 GiB memory in use. Of the allocated memory 11.19 GiB is allocated by PyTorch, and 23.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# TRUST_REMOTE_CODE=True is the key fix here\u001b[39;00m\n\u001b[32m     44\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     45\u001b[39m     model_id, \n\u001b[32m     46\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     47\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m model = \u001b[43mMistral3ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;43;03m#    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Allow the model to define its own config class\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;43;03m#    device_map=device,       # Auto-moves to GPU\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# load model in system ram then move to GPU after quantization\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb4_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPU Memory Used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.memory_allocated()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/modeling_utils.py:248\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    246\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    250\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/modeling_utils.py:4005\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4002\u001b[39m     torch.set_default_dtype(dtype_orig)\n\u001b[32m   4004\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4005\u001b[39m model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_conversions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4021\u001b[39m model.eval()  \u001b[38;5;66;03m# Set model in evaluation mode to deactivate DropOut modules by default\u001b[39;00m\n\u001b[32m   4022\u001b[39m model.set_use_kernels(use_kernels, kernel_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/modeling_utils.py:4145\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, device_mesh, weights_only, weight_mapping)\u001b[39m\n\u001b[32m   4141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeither a state dict nor checkpoint files were found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4144\u001b[39m missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n\u001b[32m-> \u001b[39m\u001b[32m4145\u001b[39m     \u001b[43mconvert_and_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmerged_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtp_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4158\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;66;03m# finally close all opened file pointers\u001b[39;00m\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_pointer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/core_model_loading.py:934\u001b[39m, in \u001b[36mconvert_and_load_state_dict_in_model\u001b[39m\u001b[34m(model, state_dict, weight_mapping, tp_plan, hf_quantizer, dtype, device_map, dtype_plan, device_mesh, disk_offload_index, disk_offload_folder)\u001b[39m\n\u001b[32m    932\u001b[39m pbar.refresh()\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     realized_value, conversion_errors = \u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m target_name, param \u001b[38;5;129;01min\u001b[39;00m realized_value.items():\n\u001b[32m    943\u001b[39m         param = param[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m param\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/core_model_loading.py:416\u001b[39m, in \u001b[36mWeightRenaming.convert\u001b[39m\u001b[34m(self, layer_name, model, config, hf_quantizer, missing_keys, conversion_errors)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    407\u001b[39m     layer_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m     \u001b[38;5;66;03m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[39;00m\n\u001b[32m    415\u001b[39m     \u001b[38;5;66;03m# attribute during the whole process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     collected_tensors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaterialize_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m     \u001b[38;5;66;03m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[39;00m\n\u001b[32m    419\u001b[39m     \u001b[38;5;66;03m# only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\u001b[39;00m\n\u001b[32m    420\u001b[39m     target_key = \u001b[38;5;28mself\u001b[39m.target_patterns[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/core_model_loading.py:391\u001b[39m, in \u001b[36mWeightTransform.materialize_tensors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Async loading\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors[\u001b[32m0\u001b[39m], Future):\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     tensors = [\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Sync loading\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(tensors[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/core_model_loading.py:530\u001b[39m, in \u001b[36mspawn_materialize.<locals>._job\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_job\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_materialize_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/core_model_loading.py:519\u001b[39m, in \u001b[36m_materialize_copy\u001b[39m\u001b[34m(tensor, device, dtype)\u001b[39m\n\u001b[32m    517\u001b[39m tensor = tensor[...]\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 136.81 MiB is free. Including non-PyTorch memory, this process has 11.43 GiB memory in use. Of the allocated memory 11.19 GiB is allocated by PyTorch, and 23.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete previous model variables if they exist\n",
    "# (Change 'model' to whatever variable name you used before)\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Force PyTorch to release cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "max_memory_mapping = {0: \"8GiB\", \"cpu\": \"48GiB\"}\n",
    "\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "# Strix Halo (8060S) works best with float16 on ROCm 6.2+\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  \n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(\"⚠️  GPU Not Detected. CPU mode.\")\n",
    "\n",
    "\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"mistralai/Ministral-3-8B-Instruct-2512\"\n",
    "model_id = \"mistralai/Ministral-3-14B-Instruct-2512-BF16\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = Mistral3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "#    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "#    device_map=device,       # Auto-moves to GPU\n",
    "    device_map=\"auto\",       # load model in system ram then move to GPU after quantization\n",
    "    quantization_config=bnb4_config,\n",
    "    max_memory=max_memory_mapping,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")\n",
    "print_model_specs(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nvidia CUDA)",
   "language": "python",
   "name": "llm-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
