{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262cc58-6c95-4fad-8da8-601902413c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e1de5d-0807-4c66-a69d-6f12b10665a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/rocm7.1\n",
      "Requirement already satisfied: torch in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (2.11.0.dev20251230+rocm7.1)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm7.1/torch-2.11.0.dev20260106%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torchvision in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (0.25.0.dev20251231+rocm7.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm7.1/torchvision-0.25.0.dev20260106%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torchaudio in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (2.10.0.dev20251231+rocm7.1)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm7.1/torchaudio-2.10.0.dev20260106%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: triton-rocm==3.6.0+git9844da95 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torch) (3.6.0+git9844da95)\n",
      "Requirement already satisfied: numpy in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torchvision) (2.3.5)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm7.1/torch-2.11.0.dev20260105%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/nightly/rocm7.1/torchvision-0.25.0.dev20260106%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/rocm7.1/torch-2.11.0.dev20260105%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl (5782.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 GB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m  \u001b[33m0:02:34\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/rocm7.1/torchaudio-2.10.0.dev20260106%2Brocm7.1-cp313-cp313-manylinux_2_28_x86_64.whl (337 kB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[2K  Attempting uninstall: torch\n",
      "\u001b[2K    Found existing installation: torch 2.11.0.dev20251230+rocm7.1\n",
      "\u001b[2K    Uninstalling torch-2.11.0.dev20251230+rocm7.1:━━━━\u001b[0m \u001b[32m0/3\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.11.0.dev20251230+rocm7.1m0/3\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: torchvision 0.25.0.dev20251231+rocm7.1rch]\n",
      "\u001b[2K    Uninstalling torchvision-0.25.0.dev20251231+rocm7.1:[32m0/3\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.25.0.dev20251231+rocm7.1torch]\n",
      "\u001b[2K  Attempting uninstall: torchaudio\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [torchvision]\n",
      "\u001b[2K    Found existing installation: torchaudio 2.10.0.dev20251231+rocm7.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [torchvision]\n",
      "\u001b[2K    Uninstalling torchaudio-2.10.0.dev20251231+rocm7.1:5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchaudio-2.10.0.dev20251231+rocm7.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [torchaudio]\n",
      "\u001b[1A\u001b[2KSuccessfully installed torch-2.11.0.dev20260105+rocm7.1 torchaudio-2.10.0.dev20260106+rocm7.1 torchvision-0.25.0.dev20260106+rocm7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee0c04-18be-480a-be75-d6bc2078e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==5.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c6e7e-dd67-44b3-a889-d84269ef2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9826183a-91bc-4413-81d8-b0f9d495bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN: h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"HF_TOKEN: {HF_TOKEN[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2dc2428-8b70-4400-acb7-dac16d72ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export HIP_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36067944-770a-42c5-8965-d514679a3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "#os.environ[\"HIP_LAUNCH_BLOCKING\"]=\"1\"\n",
    "#os.environ[\"AMD_LOG_LEVEL\"]=\"3\"\n",
    "print(os.environ.get(\"HIP_LAUNCH_BLOCKING\", \"\"))\n",
    "print(os.environ.get(\"AMD_LOG_LEVEL\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3c131f-a7f4-4400-93af-53f247ba9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.11.0.dev20260105+rocm7.1\n",
      "ROCm Version:    7.1.52802\n",
      "transformers Version:    5.0.0rc1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import TokenizersBackend\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"ROCm Version:    {torch.version.hip}\")\n",
    "print(f\"transformers Version:    {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d50a77-a1c7-4191-8ec7-71dacf24d7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import FineGrainedFP8Config\n",
    "from transformers import Mistral3ForConditionalGeneration, Mistral3Config, PixtralVisionConfig, MistralConfig\n",
    "\n",
    "#  4-Bit Quantization Config \n",
    "bnb4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "bnb8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# use for 8B model\n",
    "q8_config = FineGrainedFP8Config()\n",
    "\n",
    "\n",
    "def print_model_specs(model):\n",
    "    # 1. Get the number of parameters\n",
    "    # set only_trainable=False to include frozen weights (e.g. if using LoRA)\n",
    "    total_params = model.num_parameters(only_trainable=False)\n",
    "    \n",
    "    # 2. Get the memory footprint (size of weights in VRAM/RAM)\n",
    "    # This method is specific to Hugging Face models\n",
    "    memory_bytes = model.get_memory_footprint()\n",
    "    \n",
    "    # Format for readability\n",
    "    print(f\"Model Parameters: {total_params / 1_000_000_000:.2f} Billion\")\n",
    "    print(f\"Memory Footprint: {memory_bytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9954edad-d5e7-4fd3-a6ca-60dc8d2addec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Detected: AMD Radeon 8060S\n",
      "\n",
      "Loading mistralai/Ministral-3-3B-Instruct-2512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/transformers/modeling_utils.py:4655: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:40.)\n",
      "  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n",
      "Loading weights: 100%|██████████████████████| 822/822 [00:01<00:00, 707.08it/s, Materializing param=model.vision_tower.transformer.layers.23.ffn_norm.weight]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model Mistral3ForConditionalGeneration(\n",
      "  (model): Mistral3Model(\n",
      "    (vision_tower): PixtralVisionModel(\n",
      "      (patch_conv): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (ln_pre): PixtralRMSNorm((1024,), eps=1e-05)\n",
      "      (transformer): PixtralTransformer(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x PixtralAttentionLayer(\n",
      "            (attention_norm): PixtralRMSNorm((1024,), eps=1e-05)\n",
      "            (feed_forward): PixtralMLP(\n",
      "              (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (attention): PixtralAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (ffn_norm): PixtralRMSNorm((1024,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (patch_positional_embedding): PixtralRotaryEmbedding()\n",
      "    )\n",
      "    (multi_modal_projector): Mistral3MultiModalProjector(\n",
      "      (norm): Mistral3RMSNorm((1024,), eps=1e-05)\n",
      "      (patch_merger): Mistral3PatchMerger(\n",
      "        (merging_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (act): GELUActivation()\n",
      "      (linear_2): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "    )\n",
      "    (language_model): Ministral3Model(\n",
      "      (embed_tokens): Embedding(131072, 3072, padding_idx=11)\n",
      "      (layers): ModuleList(\n",
      "        (0-25): 26 x Ministral3DecoderLayer(\n",
      "          (self_attn): Ministral3Attention(\n",
      "            (q_proj): FP8Linear(in_features=3072, out_features=4096, bias=False)\n",
      "            (k_proj): FP8Linear(in_features=3072, out_features=1024, bias=False)\n",
      "            (v_proj): FP8Linear(in_features=3072, out_features=1024, bias=False)\n",
      "            (o_proj): FP8Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          )\n",
      "          (mlp): Ministral3MLP(\n",
      "            (gate_proj): FP8Linear(in_features=3072, out_features=9216, bias=False)\n",
      "            (up_proj): FP8Linear(in_features=3072, out_features=9216, bias=False)\n",
      "            (down_proj): FP8Linear(in_features=9216, out_features=3072, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (input_layernorm): Ministral3RMSNorm((3072,), eps=1e-05)\n",
      "          (post_attention_layernorm): Ministral3RMSNorm((3072,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): Ministral3RMSNorm((3072,), eps=1e-05)\n",
      "      (rotary_emb): Ministral3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=131072, bias=False)\n",
      ")\n",
      "GPU Memory Used: 4.36 GB\n",
      "\n",
      "Prompt: Tell me a short story.\n",
      "\n",
      "Mistral Prompt: <s>[SYSTEM_PROMPT]You are Ministral-3-3B-Instruct-2512, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\n",
      "You power an AI assistant called Le Chat.\n",
      "Your knowledge base was last updated on 2023-10-01.\n",
      "The current date is {today}.\n",
      "\n",
      "When you're not sure about some information or when the user's request requires up-to-date or specific data, you must use the available tools to fetch the information. Do not hesitate to use tools whenever they can provide a more accurate or complete response. If no relevant tools are available, then clearly state that you don't have the information and avoid making up anything.\n",
      "If the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\n",
      "You are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\n",
      "You follow these instructions in all languages, and always respond to the user in the language they use or request.\n",
      "Next sections describe the capabilities that you have.\n",
      "\n",
      "# WEB BROWSING INSTRUCTIONS\n",
      "\n",
      "You cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n",
      "\n",
      "# MULTI-MODAL INSTRUCTIONS\n",
      "\n",
      "You have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\n",
      "You cannot read nor transcribe audio files or videos.\n",
      "\n",
      "# TOOL CALLING INSTRUCTIONS\n",
      "\n",
      "You may have access to tools that you can use to fetch information or perform actions. You must use these tools in the following situations:\n",
      "\n",
      "1. When the request requires up-to-date information.\n",
      "2. When the request requires specific data that you do not have in your knowledge base.\n",
      "3. When the request involves actions that you cannot perform without tools.\n",
      "\n",
      "Always prioritize using tools to provide the most accurate and helpful response. If tools are not available, inform the user that you cannot perform the requested action at the moment.[/SYSTEM_PROMPT][INST]Tell me a short story.[/INST]\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m first_token_received = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    107\u001b[39m ttft = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfirst_token_received\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mttft\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/transformers/generation/streamers.py:226\u001b[39m, in \u001b[36mTextIteratorStreamer.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value == \u001b[38;5;28mself\u001b[39m.stop_signal:\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/queue.py:199\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete previous model variables if they exist\n",
    "# (Change 'model' to whatever variable name you used before)\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Force Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Force PyTorch to release cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#max_memory_mapping = {0: \"8GiB\", \"cpu\": \"48GiB\"}\n",
    "\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "# Strix Halo (8060S) works best with float16 on ROCm 6.2+\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  \n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(\"⚠️  GPU Not Detected. CPU mode.\")\n",
    "\n",
    "\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "#model_id = \"mistralai/Ministral-3-8B-Instruct-2512\"\n",
    "#model_id = \"mistralai/Ministral-3-14B-Instruct-2512-BF16\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = Mistral3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "    device_map=device,       # Auto-moves to GPU\n",
    "#    device_map=\"auto\",       # load model in system ram then move to GPU after quantization\n",
    "#    quantization_config=bnb4_config,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model {model}\")\n",
    "\n",
    "print(f\"GPU Memory Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#from transformers import AutoProcessor\n",
    "#processor = AutoProcessor.from_pretrained(\"model_id\")\n",
    "#inputs = processor(images=None, text=f\"{prompt}\", return_tensors=\"pt\")\n",
    "#generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "#print(\"generated ids={generate_uds}\")\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "     streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(f\"\\nMistral Prompt: {prompt}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "#print(model.generate(**generation_kwargs))\n",
    "#print(\"end generate\")\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")\n",
    "print_model_specs(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AMD ROCm 7.1)",
   "language": "python",
   "name": "llm-rocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
