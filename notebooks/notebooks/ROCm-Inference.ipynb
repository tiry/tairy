{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7b6464-978c-4e4b-b844-a36635efd9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.11.0.dev20251222+rocm7.1\n",
      "ROCm Version:    7.1.52802\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"ROCm Version:    {torch.version.hip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc6becc-06f4-4b9b-b144-0593eff68f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23f7886-b461-455f-8c3e-0fc5c4fb4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5981f531-dcb0-441e-a68b-cc0e3e3d253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral_common\n",
      "  Downloading mistral_common-1.8.8-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3.0,>=2.7 (from mistral_common)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting jsonschema>=4.21.1 (from mistral_common)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.11.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from mistral_common) (4.15.0)\n",
      "Collecting tiktoken>=0.7.0 (from mistral_common)\n",
      "  Using cached tiktoken-0.12.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from mistral_common) (12.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from mistral_common) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from mistral_common) (2.3.5)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
      "  Using cached pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0,>=2.7->mistral_common)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0,>=2.7->mistral_common)\n",
      "  Using cached pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0,>=2.7->mistral_common)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.21.1->mistral_common)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.21.1->mistral_common)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.21.1->mistral_common)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.21.1->mistral_common)\n",
      "  Using cached rpds_py-0.30.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests>=2.0.0->mistral_common) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests>=2.0.0->mistral_common) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests>=2.0.0->mistral_common) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests>=2.0.0->mistral_common) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
      "Downloading mistral_common-1.8.8-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.30.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)\n",
      "Using cached tiktoken-0.12.0-cp313-cp313-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, rpds-py, pydantic-core, pycountry, attrs, annotated-types, tiktoken, referencing, pydantic, pydantic-extra-types, jsonschema-specifications, jsonschema, mistral_common\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [mistral_common]━\u001b[0m \u001b[32m12/13\u001b[0m [mistral_common]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 attrs-25.4.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 mistral_common-1.8.8 pycountry-24.6.1 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-extra-types-2.10.6 referencing-0.37.0 rpds-py-0.30.0 tiktoken-0.12.0 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mistral_common --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112be061-ed61-4028-a161-ae7993776b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-bmoggqrv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-bmoggqrv\n",
      "  Resolved https://github.com/huggingface/transformers to commit d6a6c82680cba9c51decdacac6dd6315ea4a766a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
      "Collecting huggingface-hub<2.0,>=1.2.1 (from transformers==5.0.0.dev0)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
      "Collecting typer-slim (from transformers==5.0.0.dev0)\n",
      "  Using cached typer_slim-0.20.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.2.0)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (4.15.0)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests->transformers==5.0.0.dev0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages (from requests->transformers==5.0.0.dev0) (2.6.2)\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers==5.0.0.dev0)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.20.1-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-5.0.0.dev0-py3-none-any.whl size=11083680 sha256=8ab4dcf0a724b8adec47826c9d68b9b28ddea51fff2ef6f7fa22c17cfb04cf84\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mimuy4yl/wheels/2b/de/48/1c5b158806820c3979e1dc7a341b68dac1231f00b1a2c9442f\n",
      "Successfully built transformers\n",
      "Installing collected packages: shellingham, h11, click, anyio, typer-slim, httpcore, httpx, huggingface-hub, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [httpx]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.049;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [httpx]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [httpx]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [httpx]\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m7/9\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.3\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━\u001b[0m \u001b[32m7/9\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling transformers-4.57.3:━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m8/9\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.3━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━\u001b[0m \u001b[32m8/9\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [transformers]m━━━━\u001b[0m \u001b[32m8/9\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.12.0 click-8.3.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.2.3 shellingham-1.5.4 transformers-5.0.0.dev0 typer-slim-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21d4a4c-8c31-4043-b4ec-79dd16f97978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Detected: AMD Radeon 8060S\n",
      "\n",
      "Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|█████████████████████████████████████████████████████████████| 434/434 [00:01<00:00, 319.40it/s, Materializing param=model.norm.weight]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Tell me a short story.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:92: UserWarning: Flash Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:319.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/tiry/tairy/notebooks/venv-rocm/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:92: UserWarning: Mem Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:379.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a small village nestled among the rolling hills of a verdant land, there was a young girl named Lila who had a heart as bright and pure as the morning sun. She lived with her grandmother in a cozy cottage by the river, where they tended to an orchard filled with apple trees that whispered secrets to those who listened closely.\n",
      "\n",
      "Lila loved to explore the woods behind their home, always seeking new paths and hidden treasures. One day, while wandering through the dense forest, she stumbled upon a small, forgotten garden. It was overgrown with wildflowers and vines, but as she approached, the plants began to bend and whisper stories in her ear. The garden was alive with magic, and it seemed to call out to her.\n",
      "\n",
      "Intrigued, Lila decided to tend to the garden, clearing away the debris and nurturing the flowers back to life. As she worked, she discovered a tiny, delicate flower that no one else could see. This flower was unlike any other, its petals shimmering with a light that seemed to come from within. Lila named it \"Hope,\" for it was the hope that kept her going even on the darkest days.\n",
      "\n",
      "As Lila cared for the garden, strange things began to happen. The animals of the forest would gather around her, listening to the songs of the birds and the rustling of leaves. The villagers started to notice the change in Lila; she became more cheerful, her eyes brighter than ever\n",
      "------------------------------\n",
      "Time to First Token: 0.6499 s\n",
      "Generation Speed:    14.11 tokens/sec\n",
      "Total Tokens:        300\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "# Strix Halo (8060S) works best with float16 on ROCm 6.2+\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  \n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(\"⚠️  GPU Not Detected. CPU mode.\")\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "    device_map=device       # Auto-moves to GPU\n",
    ")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d5713-2c60-482d-82c6-a908b383e278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AMD ROCm 7.1)",
   "language": "python",
   "name": "llm-rocm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
