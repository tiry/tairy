{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4f9305-2ff2-48bc-80ca-e3bdff277648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl (184.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp313-cp313-manylinux_2_28_x86_64.whl (495 kB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [torchaudio]0m \u001b[32m 9/10\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 numpy-2.3.5 pillow-12.0.0 sympy-1.14.0 torch-2.9.1+cpu torchaudio-2.9.1+cpu torchvision-0.24.1+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from accelerate) (7.2.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from accelerate) (2.9.1+cpu)\n",
      "Requirement already satisfied: setuptools in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiry/tairy/notebooks/venv/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Using cached regex-2025.11.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, accelerate, transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [transformers]0m \u001b[32m7/8\u001b[0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch (CPU version), Transformers, and Accelerate\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1901ce-c2e2-414f-98c8-8f9f20fbd9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767aae3c04fb49108295b65d82ab510e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Tell me a short story.\n",
      "------------------------------\n",
      "Once upon a time, in a small village nestled between rolling hills and dense forests, there lived a young girl named Lila. Lila was known for her kind heart and her love of storytelling. She would often gather the children around her during the long winter evenings, weaving tales that filled their imaginations with magic and adventure.\n",
      "\n",
      "One winter, as the snow fell gently and the village wrapped itself in a blanket of white, a mysterious old man appeared at the edge of the forest. He was tall and thin, with eyes that seemed to hold secrets from another world. The villagers were wary, but Lila welcomed him warmly, inviting him to share his stories.\n",
      "\n",
      "The old man agreed, and every evening, he would come and tell tales of far-off lands, of dragons and princesses, of brave knights and wicked witches. But there was something else too; the old man spoke of hidden treasures, treasures that could grant wishes and make impossible dreams come true.\n",
      "\n",
      "Lila was fascinated by these stories and dreamed of finding such a treasure herself. One day, while exploring the woods near the village, she stumbled upon an ancient map tucked away in a hollow tree trunk. It showed a path leading deep into the heart of the forest, where the old man had said the treasure lay.\n",
      "\n",
      "Excited and eager, Lila set out on her journey. She followed the map's directions through the dense undergrowth, past shimmering streams and towering trees. As night approached, she found herself in a clearing\n",
      "------------------------------\n",
      "Time to First Token: 0.9059 s\n",
      "Generation Speed:    1.29 tokens/sec\n",
      "Total Tokens:        300\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import pipeline, TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. Setup Device (CPU)\n",
    "device = \"cpu\"\n",
    "model_id = \"gpt2\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "    device_map=device       # Auto-moves to GPU\n",
    ")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
