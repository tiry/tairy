{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3c131f-a7f4-4400-93af-53f247ba9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu128\n",
      "PyTorch CUDA Version: 12.8\n",
      "CuDNN Version:        91002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"CuDNN Version:        {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9954edad-d5e7-4fd3-a6ca-60dc8d2addec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Detected: NVIDIA GeForce RTX 4070 Ti\n",
      "\n",
      "Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a8d7f308d74d24a6316a9318cd5d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Tell me a short story.\n",
      "------------------------------\n",
      "Once upon a time, in a small village nestled between rolling hills and whispering forests, there lived an old storyteller named Gideon. Gideon was known far and wide for his tales that could make the bravest heart tremble or bring tears to the eyes of the meekest soul.\n",
      "\n",
      "One day, as Gideon sat by the fireplace in his cozy cottage, he noticed a young girl named Elara wandering lost in the woods near the village. She had a kind face and bright eyes, but she looked frightened and uncertain. Gideon, feeling the call of his calling, approached her gently.\n",
      "\n",
      "\"Elara,\" he said softly, \"why do you stray so far from the village today?\"\n",
      "\n",
      "Elara hesitated before speaking, her voice barely above a whisper. \"I'm looking for my grandmother. She disappeared a few days ago, and I've been searching everywhere.\"\n",
      "\n",
      "Moved by her plight, Gideon decided to help her. He shared stories of his own adventures and the wonders he had encountered throughout his life. Each tale brought Elara closer, and soon, she found herself entranced by the magic of storytelling.\n",
      "\n",
      "As they journeyed deeper into the woods, Gideon told her of a magical spring hidden deep within the forest, where the water held the power to heal any wound or grant wishes. This enchanted spring was guarded by a wise old owl who only allowed those with pure hearts to enter.\n",
      "\n",
      "Encouraged by Gideon’s words, Elara followed him through the dense\n",
      "------------------------------\n",
      "Time to First Token: 0.1925 s\n",
      "Generation Speed:    32.23 tokens/sec\n",
      "Total Tokens:        300\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "# Strix Halo (8060S) works best with float16 on ROCm 6.2+\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  \n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(\"⚠️  GPU Not Detected. CPU mode.\")\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "    device_map=device       # Auto-moves to GPU\n",
    ")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nvidia CUDA)",
   "language": "python",
   "name": "llm-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
