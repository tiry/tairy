{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262cc58-6c95-4fad-8da8-601902413c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cee0c04-18be-480a-be75-d6bc2078e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==5.0.0rc1\n",
      "  Downloading transformers-5.0.0rc1-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: filelock in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (3.20.1)\n",
      "Collecting huggingface-hub<2.0,>=1.2.1 (from transformers==5.0.0rc1)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (0.22.1)\n",
      "Collecting typer-slim (from transformers==5.0.0rc1)\n",
      "  Downloading typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from transformers==5.0.0rc1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (4.15.0)\n",
      "Requirement already satisfied: anyio in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0rc1) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from requests->transformers==5.0.0rc1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tiry/tairy/notebooks/venv-cuda/lib/python3.13/site-packages (from requests->transformers==5.0.0rc1) (2.6.2)\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers==5.0.0rc1)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading transformers-5.0.0rc1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: shellingham, click, typer-slim, huggingface-hub, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.3237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling transformers-4.57.3:━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.35;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [transformers]0m \u001b[32m4/5\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.1 huggingface-hub-1.2.3 shellingham-1.5.4 transformers-5.0.0rc1 typer-slim-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==5.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df7405b-7af1-4074-ae6d-434cfd27b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import TokenizersBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c131f-a7f4-4400-93af-53f247ba9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu128\n",
      "PyTorch CUDA Version: 12.8\n",
      "CuDNN Version:        91002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"CuDNN Version:        {torch.backends.cudnn.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d50a77-a1c7-4191-8ec7-71dacf24d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "#  4-Bit Quantization Config (Fits in ~6GB VRAM)\n",
    "bnb4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "bnb8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af12e6d4-ceaa-4337-9972-f5fb0e50a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mistral3ForConditionalGeneration, Mistral3Config, PixtralVisionConfig, MistralConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9954edad-d5e7-4fd3-a6ca-60dc8d2addec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Detected: NVIDIA GeForce RTX 4070 Ti\n",
      "\n",
      "Loading mistralai/Ministral-3-8B-Instruct-2512...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86523eb02dff4d8fab5042442a6726df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e702701b87e42b78347b9b1622db61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55b483d45cd4b6da22da19ac3dbe41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782974fb6dc340348000477da23a694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.mistral3.configuration_mistral3.Mistral3Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AfmoeConfig, ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, CwmConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, Lfm2MoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, Ministral3Config, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NanoChatConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, StableLmConfig, Starcoder2Config, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# TRUST_REMOTE_CODE=True is the key fix here\u001b[39;00m\n\u001b[32m     28\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     29\u001b[39m     model_id, \n\u001b[32m     30\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Standard HF uses torch_dtype, but some custom models prefer dtype\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Allow the model to define its own config class\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Auto-moves to GPU\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- 3. Run Inference ---\u001b[39;00m\n\u001b[32m     41\u001b[39m messages = [\n\u001b[32m     42\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTell me a short story.\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     43\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tairy/notebooks/venv-cuda/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:376\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m         config = config.get_text_config()\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    374\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    375\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers.models.mistral3.configuration_mistral3.Mistral3Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AfmoeConfig, ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, CwmConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, Lfm2MoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, Ministral3Config, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NanoChatConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, StableLmConfig, Starcoder2Config, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. Setup Device ---\n",
    "# Strix Halo (8060S) works best with float16 on ROCm 6.2+\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  \n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(\"⚠️  GPU Not Detected. CPU mode.\")\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model_id = \"polyverse/Meta-Llama-3.1-8B-Q5_K_M-GGUF\"\n",
    "model_id = \"mistralai/Ministral-3-3B-Instruct-2512\"\n",
    "model_id = \"mistralai/Ministral-3-8B-Instruct-2512\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "\n",
    "# TRUST_REMOTE_CODE=True is the key fix here\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=dtype,    # Standard HF uses torch_dtype, but some custom models prefer dtype\n",
    "    trust_remote_code=True, # Allow the model to define its own config class\n",
    "    device_map=device       # Auto-moves to GPU\n",
    ")\n",
    "\n",
    "# --- 3. Run Inference ---\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story.\"}\n",
    "]\n",
    "\n",
    "# Apply Mistral's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=300,    \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {messages[0]['content']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "t0 = time.time()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "# --- 4. Stream Output ---\n",
    "generated_text = \"\"\n",
    "first_token_received = False\n",
    "ttft = 0\n",
    "\n",
    "for new_text in streamer:\n",
    "    if not first_token_received:\n",
    "        ttft = time.time() - t0\n",
    "        first_token_received = True\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "    generated_text += new_text\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "# --- 5. Stats ---\n",
    "total_new_tokens = len(tokenizer.encode(generated_text))\n",
    "decoding_time = t_end - (t0 + ttft)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(f\"Time to First Token: {ttft:.4f} s\")\n",
    "if decoding_time > 0:\n",
    "    print(f\"Generation Speed:    {(total_new_tokens-1)/decoding_time:.2f} tokens/sec\")\n",
    "print(f\"Total Tokens:        {total_new_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nvidia CUDA)",
   "language": "python",
   "name": "llm-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
