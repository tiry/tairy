# vllm-bench configuration file

# Benchmark settings
benchmark:
  max_concurrency: 1
  num_prompts: 16
  temperature: 0.3
  top_p: 0.75
  request_rate: inf
  dataset_path: prompts.jsonl

# Model configurations
# Add model-specific parameters here
models:
  # Example: Ministral base model is in BF16 format
  "mistralai/Ministral-3-3B-Base-2512":
    vllm_args:
      - "--tokenizer_mode"
      - "mistral"
      - "--config_format"
      - "mistral"
      - "--load_format"
      - "mistral"
  
  # Standard models that work without additional parameters
  "google/gemma-3-4b-it":
    vllm_args:
      - "--max-num-seqs"
      - "64"
  
  "meta-llama/Llama-3.2-3B-Instruct":
    vllm_args: []
  
  "Qwen/Qwen3-4B-Instruct-2507":
    vllm_args: []
  
  # Add more models as needed
  # "organization/model-name":
  #   vllm_args:
  #     - "--additional-param"
  #     - "value"
